{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Zafonte Francesca `<s319331@studenti.polito.it>`  \n",
    "[`https://github.com/Zafonte/computational-intelligence`](https://github.com/Zafonte/computational-intelligence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic-Tac-Toe game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.isEnd = False\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        #init p1 plays first, player x. #-1 is player o\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "\n",
    "    def get_board(self):\n",
    "        return  tuple(map(tuple, self.board))\n",
    "                \n",
    "     \n",
    "    def print_Board(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n",
    "    \n",
    "\n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))\n",
    "        return positions\n",
    "    \n",
    "\n",
    "    #board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    \n",
    "    #CHECK WINNER\n",
    "    def winner(self):\n",
    "        #row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1 #x win\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1 #loss x\n",
    "            \n",
    "        #col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1 #x win\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1 #loss x\n",
    "            \n",
    "        #diagonal\n",
    "        diag1_sum = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag2_sum = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(abs(diag1_sum), abs(diag2_sum))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag1_sum == 3 or diag2_sum == 3:\n",
    "                return 1 #win x\n",
    "            else:\n",
    "                return -1 #loss x\n",
    "\n",
    "        #draw, no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        \n",
    "        #not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            return 1\n",
    "        elif result == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "    #given the position = action = tupla that indicate the row and the colum where put the simbol \n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        #switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    #costruttore\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    #Making a random action/move\n",
    "    def chooseAction(self, positions):\n",
    "        idx = np.random.choice(len(positions))\n",
    "        action = positions[idx]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning player \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlayer:\n",
    "    #learning rate. 0 < alpha < 1\n",
    "    #discounted factor - cut-down constant. 0 < gamma < 1\n",
    "    def __init__(self, name, learning_rate=0.1, discount_factor=0.5, exploration_prob=0.1):\n",
    "        self.name = name \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        #Init Q-table\n",
    "        self.Q_table = []\n",
    "   \n",
    "    \n",
    "    def chooseAction(self, positions, numberS: int):\n",
    "        #sample a float from a uniform distribution over 0 and 1    \n",
    "        if np.random.uniform(0, 1) <= self.exploration_prob: #if the sampled flaot is less than the exploration prob  \n",
    "            #the player take a random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else: #the player chooses the action with the maximum q value\n",
    "            action = max(self.Q_table[numberS,:])\n",
    "\n",
    "        return action\n",
    "           \n",
    "    \n",
    "    def update_q_table(self, numberS: int, numberA: int, reward, numberS1: int):    \n",
    "        #Compute the max value of the next state\n",
    "        max_q_next = max(self.Q_table[numberS1][numberA])\n",
    "        \n",
    "        #Update our Q-table using the Q-learning iteration\n",
    "        self.Q_table[numberS][numberA] = (1 - self.learning_rate) * self.Q_table[numberS][numberA] + self.learning_rate * (reward + self.discount_factor * max_q_next)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "availableNAction = set(range(1, 9))\n",
    "availableNBoardState = set(range(1, 10))\n",
    "mapBoard = {}\n",
    "mapAction = {}\n",
    "    \n",
    "    \n",
    "def getBoardStateNum(state: State):\n",
    "    board = frozenset(state.get_board())\n",
    "    if board not in mapBoard:\n",
    "        mapBoard[board] = choice(list(availableNBoardState)) #choose random \n",
    "        numberBS =  mapBoard[board]   \n",
    "        availableNBoardState.remove(numberBS)\n",
    "        return numberBS\n",
    "    else:\n",
    "        return mapBoard[board]  \n",
    "        \n",
    "\n",
    "def getActionNum(action):\n",
    "    action = frozenset(action)\n",
    "    if action not in mapAction:\n",
    "        mapAction[action] = choice(list(availableNAction)) #choose random \n",
    "        numberA =  mapAction[action]   \n",
    "        availableNAction.remove(numberA)\n",
    "        return numberA\n",
    "    else:\n",
    "        return mapAction[action]    \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the chosen action and returns  \n",
    "def play(state, action):\n",
    "    #take action and upate board state\n",
    "    state.updateState(action)\n",
    "    reward = state.giveReward()\n",
    "\n",
    "    \n",
    "    win = state.winner() #check winner\n",
    "    if win is not None: #if the game is end\n",
    "        return reward, state, state.isEnd\n",
    "    else:\n",
    "        #Player 2\n",
    "        positions = state.availablePositions()\n",
    "        p2_action = state.p2.chooseAction(positions)\n",
    "                       \n",
    "        state.updateState(p2_action)\n",
    "\n",
    "        return reward, state, state.isEnd    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training - RL to learn the real Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m numberS \u001b[38;5;241m=\u001b[39m  getBoardStateNum(current_state)\n\u001b[0;32m     18\u001b[0m positions \u001b[38;5;241m=\u001b[39m current_state\u001b[38;5;241m.\u001b[39mavailablePositions()\n\u001b[1;32m---> 19\u001b[0m action  \u001b[38;5;241m=\u001b[39m \u001b[43mp1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m numberA \u001b[38;5;241m=\u001b[39m getActionNum(action)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#The environment runs the chosen action \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[149], line 20\u001b[0m, in \u001b[0;36mQPlayer.chooseAction\u001b[1;34m(self, positions, numberS)\u001b[0m\n\u001b[0;32m     18\u001b[0m     action \u001b[38;5;241m=\u001b[39m positions[idx]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#the player chooses the action with the maximum q value\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumberS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#Learning the state value function:\n",
    "print(\"training...\")\n",
    "\n",
    "p1 = QPlayer(\"QPlayer\")\n",
    "p2 = RandomPlayer(\"Random\")\n",
    "\n",
    "for steps in tqdm(range(500)): \n",
    "    st = State(p1, p2)\n",
    "\n",
    "    #initialize \n",
    "    st.reset()\n",
    "        \n",
    "    for i in range(100): #until the player life is over, iterate over episodes\n",
    "        done = False\n",
    "        current_state = copy.deepcopy(st)      \n",
    "        numberS =  getBoardStateNum(current_state)\n",
    "\n",
    "        positions = current_state.availablePositions()\n",
    "        action  = p1.chooseAction(positions, numberS)\n",
    "        numberA = getActionNum(action)\n",
    "        \n",
    "        #The environment runs the chosen action \n",
    "        reward, next_state, done = play(current_state, action)\n",
    "        numberS1 =  getBoardStateNum(next_state)\n",
    "\n",
    "        p1.update_q_table(numberS, numberA, reward, numberS1)   \n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(p1.Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo - Reinforcement Learning \n",
    "Squillero's code: https://github.com/squillero/computational-intelligence/blob/master/2023-24/lab10-gx.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione della tupla denominata \"State\" con i campi 'x' e 'o'\n",
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restating the problem \n",
    "#A square array of numbers is called a magic square if the sums of the numbers in each row, each column, and both main diagonals are the same.\n",
    "#I define a magic square with sum 15 since winning in tic tac toe is equivalent to taking three numbers from a 3x3 magic square whose sum is 15(in this example). \n",
    "\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8] #Magic Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3): #row\n",
    "        for c in range(3): #columns\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x: #checks if the current position is occupied by player x\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o: #checks if the current position is occupied by player o\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "#the win function returns True if there is a combination of three elements in elements whose sum is 15\n",
    "def win(elements): \n",
    "    \"\"\"Checks if elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1 #if player x has one winning combination\n",
    "    elif win(pos.o):\n",
    "        return -1 #if player o has a winning combination\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "\n",
    "    while available:    \n",
    "        x = choice(list(available)) #choose a random element\n",
    "        state.x.add(x) #add an element x to the set x inside the state object\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available: #The overall condition is true if the player x has won or if there are no more moves available.\n",
    "            break\n",
    "\n",
    "        o = choice(list(available)) #choose a random element\n",
    "        state.o.add(o) #add an element o to the set x inside the state object\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6355.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#Player x\n",
    "\n",
    "#Learning the state value function:\n",
    "value_dictionary = defaultdict(float) #big dictionary state value during the training RL\n",
    "hit_state = defaultdict(int) \n",
    "epsilon = 0.001 #learning rate\n",
    "\n",
    "for steps in tqdm(range(50_000)):\n",
    "    trajectory = random_game() #generate a random trajectory \n",
    "    final_reward = state_value(trajectory[-1]) #compute the reward given the last element of the trajectory list(the final state of the game)\n",
    "    \n",
    "    for state in trajectory: #For each state in the trajectory\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o)) #use frozenset because they are hashable\n",
    "        hit_state[hashable_state] += 1 #to count how many times a state was visited during training\n",
    "        #updates the state rating model based on the difference between the final reward and the current rating:\n",
    "        value_dictionary[hashable_state] = value_dictionary[hashable_state] + epsilon * (final_reward - value_dictionary[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (frozenset({5}), frozenset()), Reward: 0.483826821814233\n",
      "State: (frozenset({8}), frozenset()), Reward: 0.3469893262770049\n",
      "State: (frozenset({6}), frozenset()), Reward: 0.3332456465906484\n",
      "State: (frozenset({2}), frozenset()), Reward: 0.325877932450425\n",
      "State: (frozenset({4}), frozenset()), Reward: 0.30119945258599656\n",
      "State: (frozenset({5}), frozenset({3})), Reward: 0.2941284935819315\n",
      "State: (frozenset({5}), frozenset({9})), Reward: 0.29305207043819664\n",
      "State: (frozenset({5}), frozenset({1})), Reward: 0.2789096830671588\n",
      "State: (frozenset({5}), frozenset({7})), Reward: 0.26579351800644657\n",
      "State: (frozenset({2}), frozenset({9})), Reward: 0.25423810578347694\n"
     ]
    }
   ],
   "source": [
    "#Sorted in descending order based on the value e[1], the second element of the state-value tuple, si.e. the reward\n",
    "value_dictionary_sorted = sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)\n",
    "\n",
    "#give me the first ten items\n",
    "count = 0\n",
    "for key, value in value_dictionary_sorted:\n",
    "    print(f\"State: {key}, Reward: {value}\")\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5477"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hit_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
